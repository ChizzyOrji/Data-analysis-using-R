---
title: 'Exam Template: Statistical Inference'
author: "21066371"
date: 'Jan 2022: Sep21 run'
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
# do not change these options
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE,comment=NA) # do not edit this line.
```

# Instructions to students

You should only use the file Exam_template.Rmd provided on blackboard and you should load this file from your scripts folder / directory.

Save this template as your studentID.Rmd; you will upload this file as your submission. Change the information on line 3 of this file – changing the author information to your **student ID**. Do not change the authorship to your name.

Ensure that you save your data into your data folder (as discussed in class). You may use the files mypackages.R and helperFunctions.R from blackboard. If you use these files, do not alter them. If you wish to create additional files for custom functions that you have prepared in advance, make sure that you upload these in addition to your .Rmd file and your compiled output file.

Your should knit this file to a document **Word** format.

Any changes that you make to the data (e.g. variable name changes) should be made entirely within R.

The subsubsections labelled **Answer:** indicate where you should put in your written Answers. The template also provides blank code chunks for you to complete your Answers; you may choose to add additional chunks if required.

```{r libraries, include=FALSE}
# load required libraries / additional files
if(!require(ggplot2)){install.packages("ggplot2")}
library(ggplot2)
if(!require(dplyr)){install.packages("dplyr")}
library(dplyr)
if(!require(scales)){install.packages("scales")}
library(scales)
if(!require(tidyverse)){install.packages("tidyr")}
library(tidyr)
if(!require(psych)){install.packages("psych")}
library(psych)
if(!require(boot)){install.packages("boot")}
library(boot)
if(!require(performance)){install.packages("performance")}
library(performance)
if(!require(see)){install.packages("see")}
library(see)
if(!require(patchwork)){install.packages("patchwork")}
library(patchwork)
```

```{r data}
# load dataset
setwd("C:\\DATA SCIENCE MSC\\MODULES\\STATISTICAL INFERENCE\\SI EXAM")
my_cars <- read_csv("Jan_2022_Exam_Data.csv")
```

# Data description


This dataset is part of a larger dataset that has been collected to help to estimate the price of used cars.

It contains the following variables:

- brand (manufacturer)
- model (of car)
- year (of registration of the car)
- price (in GB pounds)
- transmission (type of gearbox)
- mileage (total distance covered by the car)
- fuelType (type of fuel used by the car)
- tax (annual cost of vehicle tax)
- mpg (miles per gallon - a measure of fuel efficiency)
- engineSize (size of the engine in litres)



# Question 1: Data Preparation (11 marks)

You are interested in modelling the price of vehicles that have all of the following properties:

- mileage less than 60000
- Manual transmission
- Petrol engine (fuelType)
- Costing less than £200 in annual Vehicle Tax.

Once you have selected the rows of data with these properties, then you must *use your studentID* to select a random sample of 2000 rows of the data to perform the rest of your analysis with.

You should remove any redundant variables (where only one value remains in that variable).

This subset of the data is what you should use for the rest of this assessment. 


a. Explain what data preparation is required in order for the data in Jan_2022_Exam_Data.csv to be suitable for this analysis.

**(4 marks)**

### Answer:
The data preparation required for the data to be suitable for this analysis is as follows:
 1. Gather and save sample data; The process of data preparation starts with finding the right data sample.

2. Get to know and understand the sample data to be clear on processes the data needs to undergo to be useful for the purpose and context it is to be used.

3. Cleaning and data validation; Cleaning up the data is known to consume the most time during preparation. However, it is useful for removing faulty data and data not needed for the analysis.
Important tasks involved include:
- Removing the data considered irrelevant.
- Ensure the missing values are taken care of.
- Homogenize/Normalize the pattern of the data.
- creating relevant variables
- structuring the data

Once data is clean, it must be validated by checking for mistakes in the process used for the data preparation.

4. Transformation of the data, with enrichment; Transforming involves updating the format of the data in order to achieve a properly defined output and make it easier and faster for the data to be understood. Enrichment on the other hand involves use of related information to achieve more insights.

5. Save the data; Store the data when preparation has been concluded. This storage can be done in a 3rd party application.

6. Analysis of the final data.


b. Implement the required data preparation in the code chunk below:

**(7 marks)**

### Answer:

```{r dataprep}
view(my_cars)

my_sorted_cars <- filter(my_cars, mileage < 60000 & transmission == "Manual" & fuelType == "Petrol" & tax < 200)

# set seed before random row selection so work can be replicated
set.seed(21066371)

#select random 2000 values
n <- nrow(my_sorted_cars)
my_random_cars <- sample(1:n, size = 2000, replace = FALSE)
my_random_cars #the observation row numbers sampled

#filter the observation to get the dataframe
my_observation <- sort(my_random_cars)
filtered_random_cars <- my_sorted_cars[my_observation,]
view(filtered_random_cars)

#remove the redundant variables
cars_data <- filtered_random_cars[, c(1:4,6,8,9,10)]

```

# Question 2: Exploratory Data Analysis (22 marks)

## Descriptive Statistics

a.	What descriptive statistics would be appropriate for this dataset?  Explain why these are useful in this context.

**(2 marks)**

### Answer: 
There are different types of descriptive statistics but for my data set ones considered appropriate are:
- Measures of Central Tendency:
  This form consists majorly of the mean, median and mode of the data set.They are used to locate the distribution of   the different points in the dataset. Basically to show the response indicated the most and the average. When          working with large data, it also compresses the data set to a representative value, thereby, making it easier to      analyse. This is important since my data set is a large one
  
  The measure of central tendency descriptive statistics is useful in this context because it will allow me to compare the data in my dataset with one another.
 
- Measures of dispersion and variability of data:
This is very useful when it comes to describing the data, its spread and the variation surrounding its central    value. Different samples might have same or similar mean or median, but with totally different levels of            variability. This is useful for my data set because with standard deviation, I can tell how the data is spread. It  measures how far each observed value is from the mean. About 95% of values fall within 2 standard deviations of the
mean in most distributions.


b. Produce those descriptive statistics in the code chunk below:

**(4 marks)**

### Answer:

```{r DescriptiveStats}
#view the structure of the data set
str(cars_data)

#use the summary function to get the statistics, especially mean and median
summary(cars_data)

#use the describe function from the psych package to get the standard deviation and more insight on the data
describe(cars_data)
```

c. What have those descriptive statistics told you – and how does this inform the analysis that you would undertake on this data or any additional data cleaning requirements?

**(4 marks)**

### Answer:
From the descriptive statistics carried out using the summary function and describe function to basically determine the mean, median, standard deviation which I consider useful.

The mean shows the average value of the data set for all the variables in my data set.

The median shows the center of the data set. Comparing the median and the mean gives an idea of the data set distribution. When both values are the same, the data set is considered more or less evenly distributed, starting from the lowest to highest values. In our data, the values for both mean and median are not the same, this shows that my dataset is not evenly distributed.

The standard deviation can tell how the data is spread. It measures how far each observed value is from the mean. About 95% of values fall within 2 standard deviations of the mean in most distributions.

The relationship between the mean and standard deviation shows if the data points are far from the mean,i.e, there is a higher deviation within the data set; thus, the more spread out the data, the higher the standard deviation as can be seen in the price and mileage variables.


## Exploratory Graphs

d. What exploratory graphs would be appropriate for this dataset? Explain why these are useful in this context.

**(2 marks)**

### Answer:
The exploratory graphs to be used for this dataset are:
- Barchart; this will be used to visualize the categorical variables.
- Histogram; this will be used to visualize the one dimension numerical variables
- Scatterplot; this will be used to visualize the two dimension numerical variables


e. Now produce those exploratory graphs in the code chunk below:

**(4 marks)**

### Answer:

```{r ExploratoryGraphs}
#use the bar chart to visualise the brand and model categorical variables 
ggplot(cars_data, aes(x=brand, fill=brand)) + geom_bar()

ggplot(cars_data, aes(x=model, fill=model)) + geom_bar()

#use of histogram to visualize the numerical one dimensional variables
ggplot(cars_data, aes(x=year, col="red")) + 
  geom_histogram(color="black", fill="lightblue",
                 linetype="dashed", binwidth = 1)

ggplot(cars_data, aes(x=price)) + 
  geom_histogram(color="black", fill="lightblue",
                 linetype="dashed")

ggplot(cars_data, aes(x=mileage)) + 
  geom_histogram(color="black", fill="lightblue",
                 linetype="dashed")

ggplot(cars_data, aes(x=tax)) + 
  geom_histogram(color="black", fill="lightblue",
                 linetype="dashed")

ggplot(cars_data, aes(x=mpg)) + 
  geom_histogram(color="black", fill="lightblue",
                 linetype="dashed", binwidth = 1)

ggplot(cars_data, aes(x=engineSize)) + 
  geom_histogram(color="black", fill="lightblue",
                 linetype="dashed", binwidth = 1)

#use of scatterplot to visualize the numerical two dimensional variables
ggplot(cars_data, aes(year, price)) + geom_point(aes(colour=brand)) +
  geom_smooth(method = "lm", col = ("black"), se = FALSE) +
  ylab("Prices") +
  xlab("year") +
  ggtitle("Effect of Year On Car Prices") + scale_x_log10()
```

f. Interpret these exploratory graphs.  How do these graphs inform your subsequent analysis?

**(4 marks)**

### Answer:
The exploratory graphs above can be interpreted as follows:
- Barchart; the barchart shows that the brand of car with the highest count in the data set is focus while fiesta, focus and ecosport are the models mostly purchased as fairly used cars

- Histogram; the histogram shows the counts of cars in the data set compared to all the numerical values. The histogram for price and mileage stood out with both graphs skewed to the right. The prices of most cars were found between 10,000 to 20,000 which means most used cars fall within said price range according to the data set. Also, The milaeage of most cars fall between 10,000 and 30,000.

- Scatterplot; the scatterplot was used to visualise the relationship between year and price variables. It showed there is a strong relationship between year and price. The more recent the car year, the more expensive the car i.e, the higher the car price.


## Correlations

g. What linear correlations are present within this data?

**(2 marks)**

### Answer:


```{r linearcor}
# check for linear relationship by comparing the numerical variables with each other to determine the strongest relationship
cor(cars_data$price, cars_data$year)
cor(cars_data$year, cars_data$tax) 
cor(cars_data$year, cars_data$mileage)
cor(cars_data$year, cars_data$engineSize)
cor(cars_data$year, cars_data$mpg)
cor(cars_data$price, cars_data$tax) 
cor(cars_data$price, cars_data$mileage)
cor(cars_data$price, cars_data$engineSize)
cor(cars_data$price, cars_data$mpg)
cor(cars_data$tax, cars_data$mileage)
cor(cars_data$tax, cars_data$engineSize)
cor(cars_data$tax, cars_data$mpg)
cor(cars_data$mileage, cars_data$engineSize)
cor(cars_data$mileage, cars_data$mpg)
cor(cars_data$mpg, cars_data$engineSize)

```

# Question 3: Bivariate relationship (14 marks)

a. Which of the potential explanatory variables has the strongest linear relationship with the dependent variable?

**(1 mark)**

### Answer:
A linear relationship shows the straight line relationship that exist between two variables. The linear relationship is considered strong when the exponent (power) of the two variables in use is 1. From the linear relationship established above, the exponent power closest to 1 is 0.609. Therefore,the variables with the strongest linear relationship in my data set are Price and Year, with year as the explanatory variable and price, as the dependent variable.


b. Create a linear model to model this relationship.

**(2 marks)**

### Answer:


```{r model1}
#check for linear regression
linear_model <- lm(price ~ year, cars_data)

#obtain statistical summary of the model
summary(linear_model)

```

c. Explain and interpret the model:

**(3 marks)**

### Answer:
The linear model was used is Linear Regression and was used to model this relationship between Price and Year.

Residual is the difference between the observed and predicted values. 
The minimum value is the data point below the regression line, the 1st quartile shows that 25% of the data is less than -2718.4, the 3rd quartile shows that 25% of the data is greater than 1921.8 while the maximum value of 30015.7 is the data point above the regression line.Mean is not displayed because with linear regression, mean will always be zero.

The Estimates was used to predict the value of the response variable while the standard error is the average amount that the estimates varies from the actual value and the value should be lower than the coefficients. The Standard error is also useful for calculating the confidence interval of the estimates.
The t value checks how many standard deviations exist between the estimate and zero. The t value here has a high magnitude, therefore, the coefficient is statistically significant.

The first step in interpreting the multiple regression analysis is to examine the F-statistic and the associated p-value, at the bottom of model summary.

It can be seen that p-value of the F-statistic is < 2.2e-16, which is highly significant, this means that this model is statistically significant.


d. Comment on the performance of this model, including comments on overall model fit and the validity of model assumptions. Include any additional code required for you to make these comments in the code chunk below.

**(4 marks)**

### Answer:

After the checking the model, we realised the following graphs to test for the below:
  - Linearity
  - Homogeneity of variance
  - Influential Observations
  - Normality of Residuals to sample quantities and density

For Linearity, the reference line is expected to be flat and Horizontal but the line in our model is not. Our line is neither flat nor horizontal, hence, there's no linearity.

For homogeneity of Variance, the reference line should be flat and horizontal but the line in our graph is not, therefore there's no homogeneity.

Under Influential observations, points are expected to fall in the contour lines but not all our points fall inside this contour line, all our observations are not considered influential. Therefore, deleting certain data from this data set would have no effect on the data set.

Finally, the Normality of Residuals test analyses sample quantities against standard normal distribution quantities and density against residuals. For the former, the dots are expected to fall along the line while the later, the distribution should be near the normal curve. Since our dots and curve are very close to the expected line and normal curve, we can assume the variables are fairly distributed.

```{r model1performance}
#use the check_model function to analyse the performance of this model

check_model(linear_model)

```


## Bootstrap

e. Use bootstrapping on this model to obtain a 95% confidence interval of the estimate of the slope parameter.

**(4 marks)**

### Answer:

```{r bootstrap}
# use of Bootstrap to obtain 95% CI for R-Squared
# write a function to obtain R-Squared from the data set
my_rsquare <- function(formula, data, indices) {
  my_data <- data[indices,] # allows boot to select sample 
  model <- lm(formula, data=my_data)
  return(summary(model)$r.square)
} 
# bootstrapping with 1000 replications 
my_result <- boot(data=cars_data, statistic=my_rsquare, 
   R=1000, formula= price ~ year)

# view results
my_result 
plot(my_result)

# get 95% confidence interval 
boot.ci(my_result, type = c("norm", "basic", "perc", "stud"), 
        h = log, hdot = function(x) 1/x)

```



# Question 4: Multivariable relationship (10 marks)

Create a model with all of the appropriate remaining explanatory variables included:

```{r model2}
#check for linear regression
car_model <- lm(price ~ year + mileage + tax + mpg + engineSize, cars_data)

#obtain statistical summary of the model
summary(car_model)
```

a. Explain and interpret the model:

**(4 marks)**

### Answer:
It is important to examine the F-statistic and p-value associated with it, at the bottom of model summary.

It can be seen that p-value of the F-statistic is < 2.2e-16, which is highly significant, this means that this model is statistically significant.


b. Comment on the performance of this model, including comments on overall model fit and the validity of model assumptions. Include any additional code required for you to make these comments in the code chunk below.

**(4 marks)**

### Answer:
After the checking the model, we realised the following graphs to test for the below:
  - Linearity
  - Homogeneity of variance
  - Collinearity
  - Influential Observations
  - Normality of Residuals to sample quantities and density

For Linearity, the reference line is expected to be flat and Horizontal but the line in our model is not. Although our line does not start out horizontal and flat, it ends up so.

For homogeneity of Variance, the reference line should be flat and horizontal but the line in our graph is not, therefore there's no homogeneity.

For collinearity, the bars in our graph for the model is less than 5 which indicates there is no potential collinearity issues.

Under Influential observations, points are expected to fall in the contour lines and since all our points fall inside this contour line, all our observations are considered influential. Therefore, deleting any data from this data set would have an effect on the data set.

Finally, the Normality of Residuals test analyses sample quantities against standard normal distribution quantities and density against residuals. For the former, the dots are expected to fall along the line while the later, the distribution should be near the normal curve. Since our dots and curve are very close to the expected line and normal curve, we can assume the variables are fairly distributed.

```{r model2performance}
#use the check_model function to analyse the performance of this model

check_model(car_model)

```

c. What general concerns do you have regarding this model?

**(2 marks)**

### Answer: 
The general concerns with this model, the linear regression model include:
- It is highly limited to only Linear Relationships
- It only takes note of the Mean of the Dependent Variable
- Linear Regression can be very sensitive to the existence of outliers
- Data being used has to be Independent

# Question 5: Model simplification (8 marks)


a.	What approaches for model simplification would you consider implementing and why?

**(4 marks)**

### Answer:
This means that the model should be as simple as possible. This means there should be no redundant parameters or factor. This is achieved by fitting a maximal model followed by simplification.
The approaches for model simplification to be considered are:
- take out interaction terms.
- Remove non-significant quadratic or other non-linear terms.
- Remove non-significant explanatory variables.
- Group together factor levels that do not differ from one another.
- Amalgamate explanatory variables that have similar parameter values.
- Set non-significant slopes to zero within ANCOVA.
Balanced Realization, Truncation, Residualization, Moment Matching and Pade's approximation.

b.	What are the potential advantages of simplifying a model?

**(2 marks)**

### Answer:
The advantages of simplifying a model include:
- It gives clear exposure of errors
- it is easier to interpret the model
- the model becomes easier to use
- Enhances insights gotten from the necessary analysis
- it is easier to validate due to its simplicity

c.	 What are the potential disadvantages of simplifying a model?

**(2 marks)**

### Answer:
The disadvantages of simplifying a model include:
- Model simplification is still seen by many as an underdeveloped field/area.
- Whenever any part of a model or an entire model is altered or removed, the model changes, sometimes, the change is negative. This can also lead to loss of explanatory power.

# Question 6: Reporting (35 marks)

A client is looking to purchase a used Skoda Superb (registration year either 2018 or 2019, manual transmission, petrol engine) and wants to understand what factors influence the expected price of a used car, (and how they influence the price). 

Write a short report of 300-500 words for the client. 

Furthermore, include an explanation as to which model you would recommend, and why you have selected that model. 

Comment on any suggestions for alterations to the model that would be appropriate to consider. 

Highlight what may or may not be directly transferable from the scenario analysed in Questions 1 to 5. 


### Answer:


Purchasing the right car is an important decision that should not be made in a haste especially when the car in question is a used car. An analysis was carried out on a sample data set. This data set is part of a larger dataset that has been collected to help to estimate the prices of used cars and this analysis will provide proper guidance required to make the best decision.
The data contains specific variables used for the analysis and the variables include: 
-	brand (manufacturer)
-	model (of car)
-	year (of registration of the car)
-	price (in GB pounds)
-	transmission (type of gearbox)
-	mileage (total distance covered by the car)
-	fuelType (type of fuel used by the car) 
-	tax (annual cost of vehicle tax)
-	mpg (miles per gallon - a measure of fuel efficiency
-	engineSize (size of the engine in litres)
Overall, the entire experience was insightful. Analysing the sample data set provided a lot of insight on how so many factors affect the prices of used cars and the car registration year was seen to have the highest influence. 
The analysis on the used cars data sample showed that the more recent the registration year of a car, the more expensive the car, i.e., the higher the price. This is because most cars with recent registration years are assumed to be newer cars.
From the analysis, used cars registered between 2018 and 2019 with manual transmission, petrol engine had the highest number of used cars purchased. Another interesting fact is that mileage on cars has no influence on car prices. When mileage was compared against the car prices, the linear relationship between the two variables was very weak and this means the price of the car is not dependent on the mileage.
In conclusion, a car model registered between 2019 will be highly recommended. Even though the prices are slightly higher than those registered in 2018, they are newer, the mpg(miles per gallon) is lesser and the difference in price is minimal. However, it could be better to consider another brand of used car registered in 2019 and sold for a lesser price.


# Session Information

Do not edit this part. Make sure that you compile your document so that the information about your session (including software / package versions) is included in your submission.

```{r}
sessionInfo()
```
